{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2209a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Checking GPU Support ===\")\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "    # Clear GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\" GPU cache cleared\")\n",
    "\n",
    "else:\n",
    "    print(\"CUDA not available!\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c44ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "print(f\"PyTorch verzija: {torch.__version__}\")\n",
    "print(f\"Torchvision verzija: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d54d736",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomRotation(15),                  \n",
    "    transforms.RandomHorizontalFlip(p=0.5),          \n",
    "    transforms.RandomAffine(degrees=0, translate=(0.15, 0.15)),  \n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2),    \n",
    "    transforms.ToTensor(),  \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.1))  \n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e58901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESNET50 S TRANSFER LEARNING \n",
    "print(\"=\" * 60)\n",
    "print(\"ResNet50 Model for Alzheimer's Classification\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "class AlzheimersResNet50(nn.Module):\n",
    "    def __init__(self, num_classes=4, pretrained=True):\n",
    "        super(AlzheimersResNet50, self).__init__()\n",
    "        \n",
    "\n",
    "        self.resnet = models.resnet50(pretrained=pretrained)\n",
    "        \n",
    "\n",
    "        num_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Dropout(0.7),  \n",
    "            nn.Linear(num_features, 256),  \n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.7),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for param in self.resnet.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "    \n",
    "    def unfreeze_all(self):\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def unfreeze_last_layers(self, num_layers=2):\n",
    "        for param in self.resnet.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        if num_layers >= 1:\n",
    "            for param in self.resnet.layer4.parameters():\n",
    "                param.requires_grad = True\n",
    "                \n",
    "        if num_layers >= 2:\n",
    "            for param in self.resnet.layer3.parameters():\n",
    "                param.requires_grad = True\n",
    "                \n",
    "        if num_layers >= 3:\n",
    "            for param in self.resnet.layer2.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"Loading pre-trained ResNet50...\")\n",
    "model = AlzheimersResNet50(num_classes=4, pretrained=True)\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"ResNet50 model created!\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen parameters: {total_params - trainable_params:,}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff98934",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "\n",
    "full_dataset = datasets.ImageFolder(root='Data')\n",
    "\n",
    "\n",
    "dataset_size = len(full_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_split = int(0.70 * dataset_size)\n",
    "val_split = int(0.85 * dataset_size)\n",
    "\n",
    "train_indices = indices[:train_split]\n",
    "val_indices = indices[train_split:val_split]\n",
    "test_indices = indices[val_split:]\n",
    "\n",
    "\n",
    "train_dataset = Subset(datasets.ImageFolder(root='Data', transform=train_transform), train_indices)\n",
    "val_dataset = Subset(datasets.ImageFolder(root='Data', transform=val_transform), val_indices)\n",
    "test_dataset = Subset(datasets.ImageFolder(root='Data', transform=val_transform), test_indices)\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 32 \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "print(f\"Dataset is splitted:\")\n",
    "print(f\"   Training set: {len(train_dataset)} images ({len(train_dataset)/len(full_dataset)*100:.1f}%)\")\n",
    "print(f\"   Validation set: {len(val_dataset)} images ({len(val_dataset)/len(full_dataset)*100:.1f}%)\")\n",
    "print(f\"   Test set: {len(test_dataset)} images ({len(test_dataset)/len(full_dataset)*100:.1f}%)\")\n",
    "print(f\"    Batch size: {batch_size}\")\n",
    "\n",
    "print(f\"\\n Classes in dataset: {full_dataset.classes}\")\n",
    "print(f\" Class mapping: {full_dataset.class_to_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444ec5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SETTING UP TRAINING FOR RESNET50\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.2)  \n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.00001, weight_decay=0.05)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.3, patience=3)\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "print(f\"Loss function: CrossEntropyLoss\")\n",
    "print(f\"Optimizer: Adam (lr=0.0001)\")\n",
    "print(f\"Scheduler: ReduceLROnPlateau \")\n",
    "\n",
    "print(\"\\nTesting ResNet50 with sample batch\")\n",
    "try:\n",
    "    data_iter = iter(train_loader)\n",
    "    test_images, test_labels = next(data_iter)\n",
    "    test_images = test_images.to(device)\n",
    "    test_labels = test_labels.to(device)\n",
    "\n",
    "    print(f\"Loaded batch of {test_images.shape[0]} images\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_output = model(test_images)\n",
    "\n",
    "    print(f\"ResNet50 test passed successfully!\")\n",
    "    print(f\"Input shape: {test_images.shape}\")\n",
    "    print(f\"Output shape: {test_output.shape}\")\n",
    "    print(f\"Number of classes in output: {test_output.shape[1]}\")\n",
    "    print(f\"Sample labels: {test_labels[:5].cpu().numpy()}\")\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    print(\"ResNet50 is ready for training\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in ResNet50 model: {e}\")\n",
    "    print(f\"Error type: {type(e).__name__}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726dddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    train_bar = tqdm(train_loader, desc='Training')\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_bar):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        \n",
    "        current_acc = 100. * correct_predictions / total_samples\n",
    "        train_bar.set_postfix({\n",
    "            'Loss': f'{running_loss/(batch_idx+1):.4f}',\n",
    "            'Acc': f'{current_acc:.2f}%'\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct_predictions / total_samples\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_bar = tqdm(val_loader, desc='Validation')\n",
    "        \n",
    "        for images, labels in val_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            \n",
    "            current_acc = 100. * correct_predictions / total_samples\n",
    "            val_bar.set_postfix({\n",
    "                'Loss': f'{running_loss/(len(val_bar.iterable)):.4f}',\n",
    "                'Acc': f'{current_acc:.2f}%'\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = 100. * correct_predictions / total_samples\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346037c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, \n",
    "                num_epochs=20, device='cpu'):\n",
    "    \n",
    "    print(f\"Starting training on {device}\")\n",
    "    print(f\"Number of epochs: {num_epochs}\")\n",
    "    print(f\"Training batches: {len(train_loader)}\")\n",
    "    print(f\"Validation batches: {len(val_loader)}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(f\"\\nEPOHA {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_alzheimers_model.pth')\n",
    "            print(f\"New best model saved. Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"\\nEpoch {epoch+1} results:\")\n",
    "        print(f\"    Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"    Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "        print(f\"    Epoch time: {epoch_time:.1f}s\")\n",
    "        print(f\"    Best val accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "        if epoch > 5 and val_acc < best_val_acc - 10:\n",
    "            print(f\"Early stopping - no improvement\")\n",
    "            break\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Training completed!\")\n",
    "    print(f\"Total time: {total_time/60:.1f} minutes\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "    return train_losses, train_accuracies, val_losses, val_accuracies\n",
    "\n",
    "print(\"Main training function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5890ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "num_epochs = 15\n",
    "print(f\"Number of epochs: {num_epochs}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "\n",
    "    train_losses, train_accs, val_losses, val_accs = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        num_epochs=num_epochs,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e):\n",
    "        print(\"GPU has not enough memory!\")\n",
    "        print(\"Switching to CPU:\")\n",
    "\n",
    "        # Move model to CPU\n",
    "        device = torch.device('cpu')\n",
    "        model = model.to(device)\n",
    "        print(f\"Model moved to CPU\")\n",
    "        print(\"Training will be slower but more stable\")\n",
    "\n",
    "        # Start training on CPU\n",
    "        train_losses, train_accs, val_losses, val_accs = train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            num_epochs=num_epochs,\n",
    "            device=device\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2b828a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    print(\"Evaluating on test set:\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc='Testing'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    class_names = ['Mild Dementia', 'Moderate Dementia', 'Non Demented', 'Very mild Dementia']\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'Confusion Matrix - Accuracy: {accuracy:.2f}%')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Ucitaj najbolji model i evaluiraj\n",
    "model.load_state_dict(torch.load('best_alzheimers_model.pth'))\n",
    "test_accuracy = evaluate_model(model, test_loader, device)\n",
    "\n",
    "print(f\"Final test accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ab90f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(model, image_path, device):\n",
    "    \n",
    "    from PIL import Image\n",
    "    \n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        confidence, predicted = torch.max(probabilities, 1)\n",
    "    \n",
    "    class_names = ['Mild Dementia', 'Moderate Dementia', 'Non Demented', 'Very mild Dementia']\n",
    "    predicted_class = class_names[predicted.item()]\n",
    "    confidence_score = confidence.item() * 100\n",
    "    \n",
    "    all_probs = probabilities[0].cpu().numpy()\n",
    "    class_probabilities = {class_names[i]: all_probs[i] * 100 for i in range(len(class_names))}\n",
    "    \n",
    "    return predicted_class, confidence_score, class_probabilities\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), 'final_alzheimers_model.pth')\n",
    "print(\"Model saved as 'final_alzheimers_model.pth'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc6d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DETAILED DATASET ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import os\n",
    "import hashlib\n",
    "from PIL import Image\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "\n",
    "def get_image_hash(image_path):\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            img = img.convert('L').resize((64, 64))\n",
    "            img_array = np.array(img)\n",
    "            return hashlib.md5(img_array.tobytes()).hexdigest()\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def analyze_dataset(data_path):\n",
    "    \n",
    "    class_counts = {}\n",
    "    image_hashes = defaultdict(list)\n",
    "    total_images = 0\n",
    "    \n",
    "    \n",
    "    for class_name in os.listdir(data_path):\n",
    "        class_path = os.path.join(data_path, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            image_files = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            class_counts[class_name] = len(image_files)\n",
    "            total_images += len(image_files)\n",
    "\n",
    "            print(f\"Analyzing {class_name}: {len(image_files)} images...\")\n",
    "\n",
    "            for img_file in image_files[:500]:  \n",
    "                img_path = os.path.join(class_path, img_file)\n",
    "                img_hash = get_image_hash(img_path)\n",
    "                if img_hash:\n",
    "                    image_hashes[img_hash].append((class_name, img_file))\n",
    "    \n",
    "\n",
    "    print(f\"\\n DATASET STATISTICS:\")\n",
    "    print(f\"Total images: {total_images}\")\n",
    "    print(f\"Number of classes: {len(class_counts)}\")\n",
    "\n",
    "    print(f\"\\n CLASS DISTRIBUTION:\")\n",
    "    for class_name, count in sorted(class_counts.items()):\n",
    "        percentage = (count / total_images) * 100\n",
    "        print(f\"   {class_name}: {count:,} images ({percentage:.1f}%)\")\n",
    "\n",
    "    # Check for balance\n",
    "    counts = list(class_counts.values())\n",
    "    min_count, max_count = min(counts), max(counts)\n",
    "    imbalance_ratio = max_count / min_count\n",
    "\n",
    "    print(f\"\\n BALANCE:\")\n",
    "    print(f\"   Min images per class: {min_count:,}\")\n",
    "    print(f\"   Max images per class: {max_count:,}\")\n",
    "    print(f\"   Imbalance ratio: {imbalance_ratio:.2f}\")\n",
    "    \n",
    "    if imbalance_ratio > 10:\n",
    "        print(\"   High imbalance - class weighting or resampling\")\n",
    "    elif imbalance_ratio > 3:\n",
    "        print(\"   Moderate imbalance - class weighting\")\n",
    "    else:\n",
    "        print(\"   Dataset is fairly balanced\")\n",
    "\n",
    "    duplicates = {h: files for h, files in image_hashes.items() if len(files) > 1}\n",
    "\n",
    "    print(f\"\\n DUPLICATES:\")\n",
    "    if duplicates:\n",
    "        print(f\"   Found {len(duplicates)} groups of duplicates\")\n",
    "        total_duplicate_images = sum(len(files) for files in duplicates.values())\n",
    "        print(f\"   Total duplicate images: {total_duplicate_images}\")\n",
    "        print(\"   WARNING: Duplicates may cause data leakage!\")\n",
    "\n",
    "        for i, (hash_val, files) in enumerate(list(duplicates.items())[:3]):\n",
    "            print(f\"   Example {i+1}: {len(files)} identical images\")\n",
    "            for class_name, filename in files[:3]:\n",
    "                print(f\"      - {class_name}/{filename}\")\n",
    "    else:\n",
    "        print(\"   No duplicates found\")\n",
    "\n",
    "    return class_counts, duplicates\n",
    "\n",
    "\n",
    "try:\n",
    "    class_counts, duplicates = analyze_dataset('Data')\n",
    "except Exception as e:\n",
    "    print(f\"Error analyzing dataset: {e}\")\n",
    "    print(\"Continuing without detailed analysis...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea003b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CREATING OPTIMALLY REGULARIZED RESNET50 MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class OptimalAlzheimersResNet50(nn.Module):\n",
    "    def __init__(self, num_classes=4, pretrained=True, dropout_rate=0.5):\n",
    "        super(OptimalAlzheimersResNet50, self).__init__()\n",
    "        \n",
    "\n",
    "        self.resnet = models.resnet50(pretrained=pretrained)\n",
    "        \n",
    "\n",
    "        num_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),  # 0.5\n",
    "            \n",
    "            # Prvi FC sloj\n",
    "            nn.Linear(num_features, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate * 0.8),  # 0.4\n",
    "            \n",
    "            # Drugi FC sloj\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate * 0.6),  # 0.3\n",
    "            \n",
    "            # TreÄ‡i FC sloj\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate * 0.4),  # 0.2\n",
    "            \n",
    "            # Finalni sloj\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "        \n",
    "        self._freeze_backbone(freeze_percentage=0.7)\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.classifier.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def _freeze_backbone(self, freeze_percentage=0.7):\n",
    "        total_layers = len(list(self.resnet.parameters()))\n",
    "        layers_to_freeze = int(total_layers * freeze_percentage)\n",
    "        \n",
    "        for i, param in enumerate(self.resnet.parameters()):\n",
    "            if i < layers_to_freeze:\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Feature extraction\n",
    "        features = self.resnet(x)  # [batch_size, 2048]\n",
    "        \n",
    "        features = F.normalize(features, p=2, dim=1)\n",
    "        \n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "    \n",
    "    def unfreeze_more_layers(self, unfreeze_percentage=0.3):\n",
    "        total_layers = len(list(self.resnet.parameters()))\n",
    "        layers_to_unfreeze = int(total_layers * unfreeze_percentage)\n",
    "        \n",
    "        resnet_params = list(self.resnet.parameters())\n",
    "        for i in range(max(0, total_layers - layers_to_unfreeze), total_layers):\n",
    "            resnet_params[i].requires_grad = True\n",
    "    \n",
    "    def get_trainable_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Creating optimally regularized ResNet50\")\n",
    "optimal_model = OptimalAlzheimersResNet50(num_classes=4, dropout_rate=0.5)\n",
    "\n",
    "total_params = sum(p.numel() for p in optimal_model.parameters())\n",
    "trainable_params = optimal_model.get_trainable_params()\n",
    "\n",
    "print(f\" Total parameters: {total_params:,}\")\n",
    "print(f\" Trainable parameters: {trainable_params:,}\")\n",
    "print(f\" Frozen: {total_params - trainable_params:,} ({((total_params - trainable_params)/total_params)*100:.1f}%)\")\n",
    "print(f\" Optimal regularization: Dropout 0.5, 4 layers, BatchNorm, L2 Norm\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f606d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CREATING BALANCED TRAIN/VAL/TEST SPLIT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "\n",
    "def create_stratified_split(dataset, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, random_state=42):\n",
    "  \n",
    "    targets = [dataset[i][1] for i in range(len(dataset))]\n",
    "    indices = list(range(len(dataset)))\n",
    "    \n",
    "    # Stratified split: train + (val+test)\n",
    "    train_indices, temp_indices, train_targets, temp_targets = train_test_split(\n",
    "        indices, targets, \n",
    "        test_size=(val_ratio + test_ratio),\n",
    "        stratify=targets,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Split temp na val i test\n",
    "    val_test_ratio = val_ratio / (val_ratio + test_ratio)\n",
    "    val_indices, test_indices = train_test_split(\n",
    "        temp_indices,\n",
    "        test_size=(1 - val_test_ratio),\n",
    "        stratify=temp_targets,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    return train_indices, val_indices, test_indices\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    original_dataset = datasets.ImageFolder(root='Data')\n",
    "    \n",
    "\n",
    "    train_idx, val_idx, test_idx = create_stratified_split(\n",
    "        original_dataset,\n",
    "        train_ratio=0.70,\n",
    "        val_ratio=0.15,\n",
    "        test_ratio=0.15,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    print(f\" Stratified split created.\")\n",
    "    print(f\" Training set: {len(train_idx):,} images ({len(train_idx)/len(original_dataset)*100:.1f}%)\")\n",
    "    print(f\" Validation set: {len(val_idx):,} images ({len(val_idx)/len(original_dataset)*100:.1f}%)\")\n",
    "    print(f\" Test set: {len(test_idx):,} images ({len(test_idx)/len(original_dataset)*100:.1f}%)\")\n",
    "\n",
    "    def check_class_distribution(dataset, indices, split_name):\n",
    "        labels = [dataset[i][1] for i in indices]\n",
    "        class_counts = Counter(labels)\n",
    "        total = len(indices)\n",
    "\n",
    "        print(f\"\\n {split_name} distribution:\")\n",
    "        for class_idx, class_name in enumerate(dataset.classes):\n",
    "            count = class_counts[class_idx]\n",
    "            percentage = (count / total) * 100\n",
    "            print(f\"   {class_name}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    check_class_distribution(original_dataset, train_idx, \"TRAIN\")\n",
    "    check_class_distribution(original_dataset, val_idx, \"VALIDATION\") \n",
    "    check_class_distribution(original_dataset, test_idx, \"TEST\")\n",
    "    \n",
    "\n",
    "    print(f\"\\n Creating augmented transformations.\")\n",
    "\n",
    "    robust_train_transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.2),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.1),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "\n",
    "        transforms.RandomErasing(p=0.2, scale=(0.02, 0.15), ratio=(0.3, 3.3))\n",
    "    ])\n",
    "    \n",
    "\n",
    "    robust_val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "\n",
    "    balanced_train_dataset = Subset(\n",
    "        datasets.ImageFolder(root='Data', transform=robust_train_transform),\n",
    "        train_idx\n",
    "    )\n",
    "    \n",
    "    balanced_val_dataset = Subset(\n",
    "        datasets.ImageFolder(root='Data', transform=robust_val_transform),\n",
    "        val_idx\n",
    "    )\n",
    "    \n",
    "    balanced_test_dataset = Subset(\n",
    "        datasets.ImageFolder(root='Data', transform=robust_val_transform),\n",
    "        test_idx\n",
    "    )\n",
    "    \n",
    "    print(f\" Enhanced datasets created with stronger augmentations.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Continuing with basic split...\")\n",
    "\n",
    "    # Fallback na originalni pristup\n",
    "    balanced_train_dataset = train_dataset\n",
    "    balanced_val_dataset = val_dataset  \n",
    "    balanced_test_dataset = test_dataset\n",
    "    robust_train_transform = train_transform\n",
    "    robust_val_transform = val_transform\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f49300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SETUP FOR OPTIMAL TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, OneCycleLR\n",
    "\n",
    "def create_balanced_dataloader(dataset, batch_size=12, num_workers=0):  # Smanjeno za stabilnost\n",
    "    \n",
    "\n",
    "    targets = [dataset[i][1] for i in range(len(dataset))]\n",
    "    class_counts = Counter(targets)\n",
    "    total_samples = len(targets)\n",
    "    \n",
    "    class_weights = {cls: total_samples / count for cls, count in class_counts.items()}\n",
    "    \n",
    "\n",
    "    sample_weights = [class_weights[target] for target in targets]\n",
    "    \n",
    "\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        drop_last=True  \n",
    "    )\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\" Using device: {device}\")\n",
    "\n",
    "\n",
    "optimal_model = optimal_model.to(device)\n",
    "\n",
    "\n",
    "optimal_batch_size = 12 \n",
    "print(f\" Batch size: {optimal_batch_size}\")\n",
    "\n",
    "print(\"Creating optimal data loaders.\")\n",
    "try:\n",
    "\n",
    "    optimal_train_loader = create_balanced_dataloader(\n",
    "        balanced_train_dataset, \n",
    "        batch_size=optimal_batch_size,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "\n",
    "    optimal_val_loader = torch.utils.data.DataLoader(\n",
    "        balanced_val_dataset,\n",
    "        batch_size=optimal_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    optimal_test_loader = torch.utils.data.DataLoader(\n",
    "        balanced_test_dataset,\n",
    "        batch_size=optimal_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    print(f\" Optimal data loaders created!\")\n",
    "    print(f\"   Training batches: {len(optimal_train_loader)}\")\n",
    "    print(f\"   Validation batches: {len(optimal_val_loader)}\")\n",
    "    print(f\"   Test batches: {len(optimal_test_loader)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" Error with optimal loaders: {e}\")\n",
    "    optimal_train_loader = torch.utils.data.DataLoader(\n",
    "        balanced_train_dataset, batch_size=optimal_batch_size, \n",
    "        shuffle=True, num_workers=0\n",
    "    )\n",
    "    optimal_val_loader = torch.utils.data.DataLoader(\n",
    "        balanced_val_dataset, batch_size=optimal_batch_size, \n",
    "        shuffle=False, num_workers=0\n",
    "    )\n",
    "    optimal_test_loader = torch.utils.data.DataLoader(\n",
    "        balanced_test_dataset, batch_size=optimal_batch_size, \n",
    "        shuffle=False, num_workers=0\n",
    "    )\n",
    "\n",
    "print(\"SETTING UP OPTIMAL TRAINING\")\n",
    "# Weighted CrossEntropy with moderate label smoothing\n",
    "if 'class_counts' in locals():\n",
    "    # Calculate class weights\n",
    "    total = sum(class_counts.values())\n",
    "    weights = torch.tensor([total/count for count in class_counts.values()], \n",
    "                          dtype=torch.float32, device=device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights, label_smoothing=0.1)  # Moderate smoothing\n",
    "    print(f\" Class weights: {weights.cpu().numpy().round(2)}\")\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# AdamW optimizer \n",
    "optimizer = optim.AdamW(\n",
    "    optimal_model.parameters(),\n",
    "    lr=3e-5,  # Optimal learning rate\n",
    "    weight_decay=0.01,  # Moderate weight decay\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "# Optimal scheduler\n",
    "total_steps = len(optimal_train_loader) * 30  # 30 epoha\n",
    "scheduler = CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=len(optimal_train_loader) * 6,  # Restart svake 6 epoha\n",
    "    T_mult=1,\n",
    "    eta_min=1e-7\n",
    ")\n",
    "\n",
    "print(f\" Optimal configuration set!\")\n",
    "print(f\" Loss: CrossEntropyLoss with label smoothing 0.1\")\n",
    "print(f\" Optimizer: AdamW (lr=3e-5, wd=0.01)\")\n",
    "print(f\" Scheduler: CosineAnnealingWarmRestarts\")\n",
    "print(f\" Batch size: {optimal_batch_size}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd10c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\" PHASE 2: PROGRESSIVE UNFREEZING FOR BETTER ACCURACY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    print(f\" Checking existing variables:\")\n",
    "    print(f\" Model: {type(model).__name__}\")\n",
    "    print(f\" Device: {device}\")\n",
    "    print(f\" Train loader: {len(train_loader)} batches\")\n",
    "    print(f\" Val loader: {len(val_loader)} batches\")\n",
    "    print(\" All variables are available!\")\n",
    "except NameError as e:\n",
    "    print(f\" Missing variable: {e}\")\n",
    "    raise e\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CREATING OPTIMAL TRAINING FUNCTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "\n",
    "class OptimalTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, criterion, optimizer, scheduler, device):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        \n",
    "        # Tracking\n",
    "        self.train_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "        self.learning_rates = []\n",
    "        \n",
    "        # Early stopping - balansirano\n",
    "        self.best_val_acc = 0.0\n",
    "        self.best_model_state = None\n",
    "        self.patience_counter = 0\n",
    "        \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        train_bar = tqdm(self.train_loader, desc='Training')\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(train_bar):\n",
    "            images, labels = images.to(self.device), labels.to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "\n",
    "            l2_lambda = 0.005  # Manja nego zadnja verzija\n",
    "            l2_norm = sum(p.pow(2.0).sum() for p in self.model.parameters() if p.requires_grad)\n",
    "            loss = loss + l2_lambda * l2_norm\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            \n",
    "            current_acc = 100. * correct_predictions / total_samples\n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            train_bar.set_postfix({\n",
    "                'Loss': f'{running_loss/(batch_idx+1):.4f}',\n",
    "                'Acc': f'{current_acc:.2f}%',\n",
    "                'LR': f'{current_lr:.2e}'\n",
    "            })\n",
    "        \n",
    "        epoch_loss = running_loss / len(self.train_loader)\n",
    "        epoch_acc = 100. * correct_predictions / total_samples\n",
    "        epoch_lr = self.optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        return epoch_loss, epoch_acc, epoch_lr\n",
    "    \n",
    "    def validate_epoch(self):\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_bar = tqdm(self.val_loader, desc='Validation')\n",
    "            \n",
    "            for images, labels in val_bar:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_samples += labels.size(0)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                \n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "                current_acc = 100. * correct_predictions / total_samples\n",
    "                val_bar.set_postfix({\n",
    "                    'Loss': f'{running_loss/(len(all_predictions)//labels.size(0)):.4f}',\n",
    "                    'Acc': f'{current_acc:.2f}%'\n",
    "                })\n",
    "        \n",
    "        epoch_loss = running_loss / len(self.val_loader)\n",
    "        epoch_acc = 100. * correct_predictions / total_samples\n",
    "        \n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            all_labels, all_predictions, average=None, zero_division=0\n",
    "        )\n",
    "        \n",
    "        return epoch_loss, epoch_acc, precision, recall, f1\n",
    "    \n",
    "    def train(self, num_epochs=20, patience=8, save_path='optimal_alzheimers_model.pth'):\n",
    "        print(f\" Starting optimal training on {self.device}\")\n",
    "        print(f\" Epochs: {num_epochs}, Early stopping patience: {patience}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_start = time.time()\n",
    "            print(f\"\\n EPOHA {epoch+1}/{num_epochs}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Training\n",
    "            train_loss, train_acc, lr = self.train_epoch()\n",
    "            \n",
    "            # Validation\n",
    "            val_loss, val_acc, precision, recall, f1 = self.validate_epoch()\n",
    "            \n",
    "            # Tracking\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.train_accuracies.append(train_acc)\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.val_accuracies.append(val_acc)\n",
    "            self.learning_rates.append(lr)\n",
    "            \n",
    "            if val_acc > self.best_val_acc and (train_acc - val_acc) < 8:  # Dozvoljen gap do 8%\n",
    "                self.best_val_acc = val_acc\n",
    "                self.best_model_state = copy.deepcopy(self.model.state_dict())\n",
    "                self.patience_counter = 0\n",
    "                \n",
    "                torch.save(self.best_model_state, save_path)\n",
    "                improvement_msg = f\" New best: Model with {val_acc:.2f}% val acc saved.\"\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                improvement_msg = f\" Best: {self.best_val_acc:.2f}% (patience: {self.patience_counter}/{patience})\"\n",
    "            \n",
    "            epoch_time = time.time() - epoch_start\n",
    "            overfitting_gap = train_acc - val_acc\n",
    "            \n",
    "            print(f\"\\n RESULTS FOR EPOCH {epoch+1}:\")\n",
    "            print(f\"   Train: Loss={train_loss:.4f}, Acc={train_acc:.2f}%\")\n",
    "            print(f\"   Val:   Loss={val_loss:.4f}, Acc={val_acc:.2f}%\")\n",
    "            print(f\"   Gap: {overfitting_gap:.2f}% {'(excellent)' if overfitting_gap < 5 else '(good)' if overfitting_gap < 8 else '(caution)'}\")\n",
    "            print(f\"   Avg F1: {f1.mean():.3f}, LR: {lr:.2e}\")\n",
    "            print(f\"   Time: {epoch_time:.1f}s\")\n",
    "            print(f\"   {improvement_msg}\")\n",
    "            \n",
    "            if hasattr(self.model, 'unfreeze_more_layers'):\n",
    "                if epoch == 5 and val_acc > 80:\n",
    "                    print(f\"\\n Unfreezing more layers - Phase 1\")\n",
    "                    self.model.unfreeze_more_layers(0.3)  # 30%\n",
    "                    trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "                    print(f\"   Trainable parameters: {trainable:,}\")\n",
    "\n",
    "                elif epoch == 12 and val_acc > 85:\n",
    "                    print(f\"\\n Unfreezing more layers - Phase 2\")\n",
    "                    self.model.unfreeze_more_layers(0.5)  # 50%\n",
    "                    trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "                    print(f\"   Trainable parameters: {trainable:,}\")\n",
    "\n",
    "                    for param_group in self.optimizer.param_groups:\n",
    "                        param_group['lr'] *= 0.5\n",
    "                    print(f\"   Learning rate reduced to: {self.optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "            if self.patience_counter >= patience:\n",
    "                print(f\"\\n EARLY STOPPING after {epoch+1} epochs\")\n",
    "                print(f\"   No improvement for {patience} epochs\")\n",
    "                break\n",
    "                \n",
    "            if overfitting_gap > 15:\n",
    "                print(f\"\\n Overfitting detected --> reducing learning rate\")\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] *= 0.7\n",
    "                print(f\"   New LR: {self.optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\" Training done:\")\n",
    "        print(f\" Total time: {total_time/60:.1f} minutes\")\n",
    "        print(f\" Best validation accuracy: {self.best_val_acc:.2f}%\")\n",
    "        print(f\" Model saved to: {save_path}\")\n",
    "        \n",
    "        if self.best_val_acc >= 88:\n",
    "            print(f\" Target achieved: {self.best_val_acc:.2f}%\")\n",
    "        elif self.best_val_acc >= 85:\n",
    "            print(f\" Close to target\")\n",
    "        else:\n",
    "            print(f\" Fine-tuning needed\")\n",
    "\n",
    "\n",
    "        self.model.load_state_dict(self.best_model_state)\n",
    "        \n",
    "        return self.train_losses, self.train_accuracies, self.val_losses, self.val_accuracies\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        epochs = range(1, len(self.train_losses) + 1)\n",
    "        \n",
    "        ax1.plot(epochs, self.train_losses, 'bo-', label='Train Loss')\n",
    "        ax1.plot(epochs, self.val_losses, 'ro-', label='Val Loss')\n",
    "        ax1.set_title('Training and Validation Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Accuracy\n",
    "        ax2.plot(epochs, self.train_accuracies, 'bo-', label='Train Acc')\n",
    "        ax2.plot(epochs, self.val_accuracies, 'ro-', label='Val Acc')\n",
    "        ax2.axhline(y=88, color='g', linestyle='--', alpha=0.7)\n",
    "        ax2.axhline(y=90, color='orange', linestyle='--', alpha=0.7)\n",
    "        ax2.set_title('Training and Validation Accuracy')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy (%)')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        # Learning Rate\n",
    "        ax3.plot(epochs, self.learning_rates, 'go-', label='Learning Rate')\n",
    "        ax3.set_title('Learning Rate Schedule')\n",
    "        ax3.set_xlabel('Epoch')\n",
    "        ax3.set_ylabel('Learning Rate')\n",
    "        ax3.set_yscale('log')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True)\n",
    "        \n",
    "        # Overfitting check\n",
    "        train_val_diff = [t - v for t, v in zip(self.train_accuracies, self.val_accuracies)]\n",
    "        ax4.plot(epochs, train_val_diff, 'mo-', label='Train - Val Acc')\n",
    "        ax4.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "        ax4.axhline(y=5, color='g', linestyle='--', alpha=0.5)\n",
    "        ax4.axhline(y=8, color='orange', linestyle='--', alpha=0.5)\n",
    "        ax4.set_title('Overfitting Monitor')\n",
    "        ax4.set_xlabel('Epoch')  \n",
    "        ax4.set_ylabel('Accuracy Difference (%)')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "progressive_optimizer = optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.01)\n",
    "progressive_scheduler = optim.lr_scheduler.CosineAnnealingLR(progressive_optimizer, T_max=20)\n",
    "\n",
    "\n",
    "progressive_trainer = OptimalTrainer(\n",
    "    model=model,  # Koristi postojeÄ‡i model\n",
    "    train_loader=train_loader,  # Koristi postojeÄ‡i train_loader\n",
    "    val_loader=val_loader,  # Koristi postojeÄ‡i val_loader\n",
    "    criterion=criterion,  # Koristi postojeÄ‡i criterion\n",
    "    optimizer=progressive_optimizer,\n",
    "    scheduler=progressive_scheduler,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\" Progressive Trainer done.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da61d83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" STARTING OPTIMAL TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# Optimalni parametri\n",
    "optimal_epochs = 30\n",
    "optimal_patience = 10\n",
    "\n",
    "print(f\" OPTIMAL PARAMETERS:\")\n",
    "print(f\"   Epochs: {optimal_epochs}\")\n",
    "print(f\"   Early stopping patience: {optimal_patience}\")\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "print(f\"   Learning rate: 3e-5\")\n",
    "print(f\"   Weight decay: 0.01\")\n",
    "print(f\"   Dropout: 0.5\")\n",
    "print(f\"   Gradient clipping: 1.0\")\n",
    "\n",
    "\n",
    "# PoÄisti memoriju od starih modela\n",
    "if 'model' in globals():\n",
    "    del model\n",
    "if 'ultra_model' in globals():\n",
    "    del ultra_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "response = input(\"\\\\n Starting optimal training (y/n): \")\n",
    "\n",
    "if response.lower() in ['y', 'yes', 'da', 'd']:\n",
    "    print(f\"\\\\n Optimal training starting:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Pokreni optimalno treniranje\n",
    "        optimal_train_hist, optimal_train_acc_hist, optimal_val_hist, optimal_val_acc_hist = optimal_trainer.train(\n",
    "            num_epochs=optimal_epochs,\n",
    "            patience=optimal_patience,\n",
    "            save_path='optimal_alzheimers_model.pth'\n",
    "        )\n",
    "\n",
    "        print(\"\\\\n Optimal training finished.\")\n",
    "\n",
    "        # Analyze results\n",
    "        final_val_acc = optimal_val_acc_hist[-1] if optimal_val_acc_hist else 0\n",
    "        best_val_acc = max(optimal_val_acc_hist) if optimal_val_acc_hist else 0\n",
    "        final_train_acc = optimal_train_acc_hist[-1] if optimal_train_acc_hist else 0\n",
    "        overfitting_gap = final_train_acc - final_val_acc\n",
    "\n",
    "        print(f\"\\\\n FINAL RESULTS:\")\n",
    "        print(f\"   Best Val Accuracy: {best_val_acc:.2f}%\")\n",
    "        print(f\"   Final Val Accuracy: {final_val_acc:.2f}%\")\n",
    "        print(f\"   Overfitting gap: {overfitting_gap:.2f}%\")\n",
    "        \n",
    "        \n",
    "\n",
    "        print(\"\\\\n Saving and plotting training history...\")\n",
    "        optimal_trainer.plot_training_history()\n",
    "        \n",
    "\n",
    "        torch.save(optimal_model.state_dict(), 'final_optimal_alzheimers_model.pth')\n",
    "        print(f\"\\\\n Final model saved as 'final_optimal_alzheimers_model.pth'\")\n",
    "\n",
    "        # Test on test set\n",
    "        print(\"\\\\n Testing on test set...\")\n",
    "        optimal_model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        all_test_preds = []\n",
    "        all_test_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in optimal_test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = optimal_model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "                all_test_preds.extend(predicted.cpu().numpy())\n",
    "                all_test_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        test_accuracy = 100 * test_correct / test_total\n",
    "        print(f\"FINAL TEST ACCURACY: {test_accuracy:.2f}%\")\n",
    "        \n",
    "        # Per-class test rezultati\n",
    "        from sklearn.metrics import classification_report\n",
    "        class_names = ['Mild Dementia', 'Moderate Dementia', 'Non Demented', 'Very mild Dementia']\n",
    "        print(\"\\\\nPer-class test results:\")\n",
    "        print(classification_report(all_test_labels, all_test_preds, target_names=class_names))\n",
    "\n",
    "        # Save per-class results\n",
    "        with open(\"per_class_results.txt\", \"w\") as f:\n",
    "            f.write(classification_report(all_test_labels, all_test_preds, target_names=class_names))\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\\\n Training interrupted by user\")\n",
    "        print(\"Model has been saved so far...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\\\n Error during training: {e}\")\n",
    "        print(\"Checking if all parameters are set correctly...\")\n",
    "        \n",
    "        # Debugging informacije\n",
    "        print(f\"\\\\n DEBUGGING INFO:\")\n",
    "        print(f\"   Device: {device}\")\n",
    "        print(f\"   Model parameters: {optimal_model.get_trainable_params():,}\")\n",
    "        print(f\"   Train loader batches: {len(optimal_train_loader)}\")\n",
    "        print(f\"   Val loader batches: {len(optimal_val_loader)}\")\n",
    "        \n",
    "\n",
    "        if \"cuda\" in str(e).lower() or \"memory\" in str(e).lower():\n",
    "            print(\"\\\\n Switching to CPU due to GPU error...\")\n",
    "            device = torch.device('cpu')\n",
    "            optimal_model = optimal_model.to(device)\n",
    "            \n",
    "            # Update trainer device\n",
    "            optimal_trainer.device = device\n",
    "            optimal_trainer.model = optimal_model\n",
    "\n",
    "            print(\"Trying again with CPU\")\n",
    "            optimal_train_hist, optimal_train_acc_hist, optimal_val_hist, optimal_val_acc_hist = optimal_trainer.train(\n",
    "                num_epochs=optimal_epochs,\n",
    "                patience=optimal_patience,\n",
    "                save_path='optimal_alzheimers_model_cpu.pth'\n",
    "            )\n",
    "            \n",
    "else:\n",
    "    print(\"\\\\n Training postponed.\")\n",
    "    print(\"\\\\n Currently have:\")\n",
    "    print(\"   - Optimal regularized model (dropout 0.5, 4 layers)\")\n",
    "    print(\"   - Optimal learning rate (3e-5)\")\n",
    "    print(\"   - Moderate weight decay (0.01)\")\n",
    "    print(\"   - Optimal batch size (12) - reduced for stability\")\n",
    "    print(\"   - 70% layers frozen\")\n",
    "    print(\"   - Gradual fine-tuning (epoch 8 and 16)\")\n",
    "    print(\"\\\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a855a87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMAL MODEL USING SGD OPTIMIZER \n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CREATING OPTIMAL ALZHEIMERS MODEL USING SGD OPTIMIZER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# SGD Model s optimalnim parametrima\n",
    "sgd_model = AlzheimersResNet50(num_classes=4, dropout_rate=0.5).to(device)\n",
    "print(f\"Model created on: {device}\")\n",
    "\n",
    "# SGD Optimizer \n",
    "sgd_optimizer = torch.optim.SGD(\n",
    "    sgd_model.parameters(),\n",
    "    lr=0.01,              # VeÄ‡i learning rate za SGD\n",
    "    momentum=0.9,         # KljuÄno za SGD performanse\n",
    "    weight_decay=1e-4,    # L2 regularization\n",
    "    nesterov=True         # Nesterov momentum za brÅ¾i i stabilniji trening\n",
    ")\n",
    "\n",
    "# Isti scheduler kao i za AdamW\n",
    "sgd_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    sgd_optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f843b969",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"PHASE 2: PROGRESSIVE UNFREEZING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "# Step 1: Unfreeze layer4 (last ResNet block)\n",
    "print(\"\\n STEP 1: Unfreezing layer4 (last ResNet block)\")\n",
    "sgd_model.unfreeze_more_layers(0.3)  # Unfreeze 30% of layers\n",
    "\n",
    "# Count parameters after unfreezing\n",
    "total_params = sum(p.numel() for p in sgd_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in sgd_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,} (before: ~6,000)\")\n",
    "print(f\"Trainable ratio: {trainable_params/total_params*100:.1f}%\")\n",
    "\n",
    "# Reduce learning rate for fine-tuning\n",
    "print(f\"\\n Adjusting optimizer for fine-tuning:\")\n",
    "print(f\"Learning rate: 0.00001 â†’ 0.000005 (lowered for stability)\")\n",
    "\n",
    "# New optimizer with lower LR\n",
    "optimizer = optim.AdamW(sgd_model.parameters(), lr=0.000005, weight_decay=0.05)\n",
    "\n",
    "# Reduce number of epochs - fine-tuning is faster\n",
    "fine_tuning_epochs = 10\n",
    "\n",
    "print(f\"Epochs for fine-tuning: {fine_tuning_epochs}\")\n",
    "print(f\"\\nStarting fine-tuning training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac22f41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"STARTING FINE-TUNING TRAINING WITH PROGRESSIVE UNFREEZING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Scheduler za fine-tuning\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.7, patience=3\n",
    ")\n",
    "\n",
    "# Training tracking\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "best_val_acc = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "print(f\"Model: {sgd_model.__class__.__name__}\")\n",
    "print(f\"Optimizer: AdamW (LR: {optimizer.param_groups[0]['lr']:.2e})\")\n",
    "print(f\"Trainable params: {trainable_params:,} ({trainable_params/total_params*100:.1f}%)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# FINE-TUNING TRAINING LOOP\n",
    "for epoch in range(fine_tuning_epochs):\n",
    "    epoch_start = time.time()\n",
    "    print(f\"\\nEPOCH {epoch+1}/{fine_tuning_epochs}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # TRAINING PHASE\n",
    "    sgd_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    train_bar = tqdm(train_loader, desc='Training')\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_bar):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = sgd_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping za stabilnost\n",
    "        torch.nn.utils.clip_grad_norm_(sgd_model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Progress update\n",
    "        current_acc = 100. * correct_predictions / total_samples\n",
    "        train_bar.set_postfix({\n",
    "            'Loss': f'{running_loss/(batch_idx+1):.4f}',\n",
    "            'Acc': f'{current_acc:.2f}%'\n",
    "        })\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = 100. * correct_predictions / total_samples\n",
    "    \n",
    "    # VALIDATION PHASE\n",
    "    sgd_model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_bar = tqdm(val_loader, desc='Validation')\n",
    "        \n",
    "        for images, labels in val_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = sgd_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            current_acc = 100. * val_correct / val_total\n",
    "            val_bar.set_postfix({'Acc': f'{current_acc:.2f}%'})\n",
    "    \n",
    "    val_loss = val_running_loss / len(val_loader)\n",
    "    val_acc = 100. * val_correct / val_total\n",
    "    \n",
    "    # Save tracking\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    # Check for best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = copy.deepcopy(sgd_model.state_dict())\n",
    "        improvement_msg = \"Best new record\"\n",
    "    else:\n",
    "        improvement_msg = f\"Best: {best_val_acc:.2f}%\"\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_acc)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Results summary\n",
    "    overfitting_gap = train_acc - val_acc\n",
    "    epoch_time = time.time() - epoch_start\n",
    "\n",
    "    print(f\"\\nEpoch results {epoch+1}:\")\n",
    "    print(f\"  Train: Loss={train_loss:.4f}, Acc={train_acc:.2f}%\")\n",
    "    print(f\"  Val:   Loss={val_loss:.4f}, Acc={val_acc:.2f}%\")\n",
    "    print(f\"  Gap: {overfitting_gap:.2f}% {'(excellent)' if overfitting_gap < 5 else '(good)' if overfitting_gap < 8 else '(caution)'}\")\n",
    "    print(f\"  LR: {current_lr:.2e}, Time: {epoch_time:.1f}s\")\n",
    "    print(f\"  {improvement_msg}\")\n",
    "    \n",
    "    # Progressive unfreezing na polovini treninga\n",
    "    if epoch == 5 and val_acc > 82:\n",
    "        print(f\"\\nUnfreezing additional layers - Phase 2!\")\n",
    "        sgd_model.unfreeze_more_layers(0.5)  # 50% slojeva\n",
    "        new_trainable = sum(p.numel() for p in sgd_model.parameters() if p.requires_grad)\n",
    "        print(f\"  New trainable parameters: {new_trainable:,}\")\n",
    "\n",
    "        # Reduce learning rate for fine-tuning\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= 0.8\n",
    "        print(f\"  Learning rate reduced to: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINE-TUNING COMPLETED!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total time: {total_time/60:.1f} minutes\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "\n",
    "# Load best model\n",
    "if best_model_state:\n",
    "    sgd_model.load_state_dict(best_model_state)\n",
    "    print(\"Best model loaded!\")\n",
    "\n",
    "print(f\"\\nFINAL RESULTS:\")\n",
    "print(f\"  Starting accuracy: 78.33%\")\n",
    "print(f\"  Final accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"  Improvement: +{best_val_acc - 78.33:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eb8fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TESTING FINAL MODEL ON TEST DATASET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Definiraj nazive klasa\n",
    "class_names = ['Mild Dementia', 'Moderate Dementia', 'Non Demented', 'Very mild Dementia']\n",
    "\n",
    "print(f\"Model: {sgd_model.__class__.__name__}\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"Test dataset: {len(test_loader)} batches, {len(test_dataset)} images\")\n",
    "\n",
    "# TESTING ON TEST SET\n",
    "sgd_model.eval()\n",
    "test_predictions = []\n",
    "test_labels = []\n",
    "test_probabilities = []\n",
    "\n",
    "print(\"\\nStarting testing.\")\n",
    "with torch.no_grad():\n",
    "    test_bar = tqdm(test_loader, desc='Testiranje')\n",
    "    \n",
    "    for images, labels in test_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = sgd_model(images)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        test_predictions.extend(predicted.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        test_probabilities.extend(probabilities.cpu().numpy())\n",
    "\n",
    "\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions) * 100\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    test_labels, test_predictions, average=None, zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESULTS OF TESTING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"Number of test images: {len(test_labels)}\")\n",
    "\n",
    "# Per-class results\n",
    "print(f\"\\nPer-class results:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"  {class_name}:\")\n",
    "    print(f\"    Precision: {precision[i]:.3f}\")\n",
    "    print(f\"    Recall: {recall[i]:.3f}\")\n",
    "    print(f\"    F1-Score: {f1[i]:.3f}\")\n",
    "    print(f\"    Support: {support[i]} slika\")\n",
    "\n",
    "\n",
    "avg_precision = np.mean(precision)\n",
    "avg_recall = np.mean(recall)\n",
    "avg_f1 = np.mean(f1)\n",
    "\n",
    "print(f\"\\nAverage metrics:\")\n",
    "print(f\"  Average Precision: {avg_precision:.3f}\")\n",
    "print(f\"  Average Recall: {avg_recall:.3f}\")\n",
    "print(f\"  Average F1-Score: {avg_f1:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY OF PERFORMANCE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"Difference Val-Test: {best_val_acc - test_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "print(f\"\\nFINAL RESULTS:\")\n",
    "print(f\"   Starting accuracy: 78.33%\")\n",
    "print(f\"   Final test accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"   Improvement: +{test_accuracy - 78.33:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2355219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFUSION MATRIX AND DETAILED VISUALIZATIONS\n",
    "print(\"=\" * 80)\n",
    "print(\"CONFUSION MATRIX AND DETAILED VISUALIZATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Kreiraj confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_predictions)\n",
    "cm_normalized = confusion_matrix(test_labels, test_predictions, normalize='true')\n",
    "\n",
    "# Nastavi figure sa subplotovima\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Detaljne Analize Modela - Test Rezultati', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Confusion Matrix (apsolutne vrijednosti)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            ax=axes[0,0])\n",
    "axes[0,0].set_title('Confusion Matrix (Brojevi)')\n",
    "axes[0,0].set_ylabel('Stvarne Klase')\n",
    "axes[0,0].set_xlabel('Predvidene Klase')\n",
    "\n",
    "# 2. Confusion Matrix (postoci)\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Oranges',\n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            ax=axes[0,1])\n",
    "axes[0,1].set_title('Confusion Matrix (Postoci)')\n",
    "axes[0,1].set_ylabel('Stvarne Klase')\n",
    "axes[0,1].set_xlabel('Predvidene Klase')\n",
    "\n",
    "# 3. Per-class metrije\n",
    "metrics_data = {\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1\n",
    "}\n",
    "x_pos = np.arange(len(class_names))\n",
    "bar_width = 0.25\n",
    "\n",
    "for i, (metric_name, values) in enumerate(metrics_data.items()):\n",
    "    axes[1,0].bar(x_pos + i*bar_width, values, bar_width, \n",
    "                  label=metric_name, alpha=0.8)\n",
    "\n",
    "axes[1,0].set_xlabel('Klase')\n",
    "axes[1,0].set_ylabel('Score')\n",
    "axes[1,0].set_title('Metrije po Klasama')\n",
    "axes[1,0].set_xticks(x_pos + bar_width)\n",
    "axes[1,0].set_xticklabels([name.split()[0] for name in class_names], rotation=45)\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Training progress\n",
    "if 'val_accuracies' in locals() and len(val_accuracies) > 0:\n",
    "    epochs = range(1, len(val_accuracies) + 1)\n",
    "    axes[1,1].plot(epochs, val_accuracies, 'o-', color='red', linewidth=2, \n",
    "                   markersize=6, label=f'Validation (Max: {max(val_accuracies):.2f}%)')\n",
    "    if 'train_accuracies' in locals():\n",
    "        axes[1,1].plot(epochs, train_accuracies, 's-', color='blue', linewidth=2,\n",
    "                       markersize=4, alpha=0.7, label=f'Training (Final: {train_accuracies[-1]:.2f}%)')\n",
    "    \n",
    "    axes[1,1].axhline(y=88, color='green', linestyle='--', alpha=0.7, label='Cilj (88%)')\n",
    "    axes[1,1].axhline(y=90, color='orange', linestyle='--', alpha=0.7, label='Izvrsno (90%)')\n",
    "    axes[1,1].set_xlabel('Epoha')\n",
    "    axes[1,1].set_ylabel('Accuracy (%)')\n",
    "    axes[1,1].set_title('Training Progress')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    # Ako nemamo training podatke, prikazi sazetak\n",
    "    summary_text = f\"\"\"\n",
    "    SAZETAK REZULTATA:\n",
    "    \n",
    "    Validation Accuracy: {best_val_acc:.2f}%\n",
    "    Test Accuracy: {test_accuracy:.2f}%\n",
    "    \n",
    "    Poboljsanje: +{test_accuracy - 78.33:.2f}%\n",
    "    \n",
    "    Model Status: {'PRODUKCIJSKI READY' if test_accuracy >= 85 else 'POTREBNO PODESAVANJE'}\n",
    "    \"\"\"\n",
    "    axes[1,1].text(0.1, 0.5, summary_text, transform=axes[1,1].transAxes,\n",
    "                   fontsize=12, verticalalignment='center',\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
    "    axes[1,1].set_title('Sazetak Performansi')\n",
    "    axes[1,1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Dodatne analize\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DODATNE ANALIZE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Analiza gresaka\n",
    "print(\"Najcesce greske:\")\n",
    "for i in range(len(class_names)):\n",
    "    for j in range(len(class_names)):\n",
    "        if i != j and cm[i,j] > 0:\n",
    "            error_rate = cm[i,j] / np.sum(cm[i,:]) * 100\n",
    "            if error_rate > 5:  # Prikazi samo znacajne greske\n",
    "                print(f\"  {class_names[i]} â†’ {class_names[j]}: {cm[i,j]} slika ({error_rate:.1f}%)\")\n",
    "\n",
    "# Najbolja i najslabija klasa\n",
    "best_class_idx = np.argmax(f1)\n",
    "worst_class_idx = np.argmin(f1)\n",
    "\n",
    "print(f\"\\nNajbolja klasa: {class_names[best_class_idx]} (F1: {f1[best_class_idx]:.3f})\")\n",
    "print(f\"Najslabija klasa: {class_names[worst_class_idx]} (F1: {f1[worst_class_idx]:.3f})\")\n",
    "\n",
    "# Model robusnos\n",
    "confidence_scores = np.max(test_probabilities, axis=1)\n",
    "avg_confidence = np.mean(confidence_scores)\n",
    "low_confidence_count = np.sum(confidence_scores < 0.7)\n",
    "\n",
    "print(f\"\\nModel pouzdanost:\")\n",
    "print(f\"  Prosjecna pouzdanost: {avg_confidence:.3f}\")\n",
    "print(f\"  Broj predikcija s pouzdanosti < 70%: {low_confidence_count}/{len(confidence_scores)} ({low_confidence_count/len(confidence_scores)*100:.1f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61efaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"ANALYSIS OF PERFORMANCE BY CLASS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Generiraj classification report\n",
    "report = classification_report(test_labels, test_predictions, \n",
    "                             target_names=class_names, output_dict=True)\n",
    "\n",
    "print(\"CLASSIFICATION REPORT:\")\n",
    "print(classification_report(test_labels, test_predictions, target_names=class_names))\n",
    "\n",
    "# Detailed analysis by class\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DETAILED ANALYSIS BY CLASS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"\\nCLASS: {class_name}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Basic statistics\n",
    "    tp = cm[i, i]  # True positives\n",
    "    fp = np.sum(cm[:, i]) - tp  # False positives\n",
    "    fn = np.sum(cm[i, :]) - tp  # False negatives\n",
    "    tn = np.sum(cm) - tp - fp - fn  # True negatives\n",
    "    \n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    sensitivity = recall[i]  # Recall = Sensitivity\n",
    "\n",
    "    print(f\"  Basic Statistics:\")\n",
    "    print(f\"    Number of test images: {support[i]}\")\n",
    "    print(f\"    Correctly classified: {tp}\")\n",
    "    print(f\"    Misclassified as this class: {fp}\")\n",
    "    print(f\"    Missed: {fn}\")\n",
    "\n",
    "    print(f\"  Metrics:\")\n",
    "    print(f\"    Precision: {precision[i]:.3f} ({precision[i]*100:.1f}%)\")\n",
    "    print(f\"    Recall (Sensitivity): {recall[i]:.3f} ({recall[i]*100:.1f}%)\")\n",
    "    print(f\"    F1-Score: {f1[i]:.3f}\")\n",
    "    print(f\"    Specificity: {specificity:.3f} ({specificity*100:.1f}%)\")\n",
    "    \n",
    "   \n",
    "\n",
    "    if precision[i] < 0.8 and recall[i] >= 0.8:\n",
    "        print(f\"    High false positives - model is diagnosing {class_name} too lately\")\n",
    "    elif precision[i] >= 0.8 and recall[i] < 0.8:\n",
    "        print(f\"    High false negatives - model is missing cases of {class_name}\")\n",
    "    elif precision[i] < 0.8 and recall[i] < 0.8:\n",
    "        print(f\"    More data or better features needed for {class_name}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANALYSIS OF MOST COMMON ERRORS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "confusion_pairs = []\n",
    "for i in range(len(class_names)):\n",
    "    for j in range(len(class_names)):\n",
    "        if i != j and cm[i,j] > 0:\n",
    "            error_rate = cm[i,j] / np.sum(cm[i,:])\n",
    "            confusion_pairs.append((error_rate, cm[i,j], i, j))\n",
    "\n",
    "# Sortiraj po error rate\n",
    "confusion_pairs.sort(reverse=True)\n",
    "\n",
    "print(\"Top 5 most problematic pairs:\")\n",
    "for idx, (error_rate, count, i, j) in enumerate(confusion_pairs[:5]):\n",
    "    print(f\"{idx+1}. {class_names[i]} â†’ {class_names[j]}\")\n",
    "    print(f\"   Number of errors: {count}\")\n",
    "    print(f\"   Error rate: {error_rate*100:.1f}%\")\n",
    "\n",
    "    # Medicinsko objasnjenje zasto se mogu zamijeniti\n",
    "    if 'Mild' in class_names[i] and 'Very mild' in class_names[j]:\n",
    "        print(\"   Reason: Close stages of dementia - hard to distinguish\")\n",
    "    elif 'Non Demented' in class_names[i] and 'Very mild' in class_names[j]:\n",
    "        print(\"   Reason: Early stages may resemble normal conditions\")\n",
    "    elif 'Moderate' in class_names[i] and 'Mild' in class_names[j]:\n",
    "        print(\"   Reason: Overlapping symptoms between stages\")\n",
    "    print()\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"OVERALL MODEL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Precision': np.mean(precision),\n",
    "    'Recall': np.mean(recall), \n",
    "    'F1-Score': np.mean(f1),\n",
    "    'Accuracy': test_accuracy/100\n",
    "}\n",
    "\n",
    "print(\"Average performance:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.3f} ({value*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nModel Status:\")\n",
    "if test_accuracy >= 90:\n",
    "    print(\"   Model is at the cutting edge of current research!\")\n",
    "elif test_accuracy >= 85:\n",
    "    print(\"   Model is ready for real-world use!\")\n",
    "elif test_accuracy >= 80:\n",
    "    print(\"   Solid performance, potential for further optimization\")\n",
    "else:\n",
    "    print(\"   Further optimization needed\")\n",
    "\n",
    "print(f\"\\nProgress:\")\n",
    "print(f\"  Initial accuracy: 78.33%\")\n",
    "print(f\"  Final accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"  Improvement: +{test_accuracy-78.33:.2f} percentage points\")\n",
    "\n",
    "print(f\"\\nMedical Applicability:\")\n",
    "if avg_metrics['F1-Score'] >= 0.9:\n",
    "    print(\"  EXCELLENT for aiding diagnosis\")\n",
    "elif avg_metrics['F1-Score'] >= 0.8:\n",
    "    print(\"  VERY GOOD for screening\")\n",
    "elif avg_metrics['F1-Score'] >= 0.7:\n",
    "    print(\"  USEFUL as a supportive tool\")\n",
    "else:\n",
    "    print(\"  Further optimization needed for clinical use\")\n",
    "\n",
    "print(\"\\nModel has been successfully developed and tested!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6c5ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNIMANJE FINALNOG MODELA I KREIRANJE SAZETKA\n",
    "print(\"=\" * 80)\n",
    "print(\"SNIMANJE MODELA I KREIRANJE FINALNOG SAZETKA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "import os\n",
    "model_dir = \"saved_models\"\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "    print(f\"Kreiran direktorij: {model_dir}\")\n",
    "\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_name = f\"alzheimers_resnet50_{timestamp}\"\n",
    "\n",
    "\n",
    "model_path = os.path.join(model_dir, f\"{model_name}.pth\")\n",
    "torch.save({\n",
    "    'model_state_dict': sgd_model.state_dict(),\n",
    "    'model_class': 'AlzheimersResNet50',\n",
    "    'num_classes': 4,\n",
    "    'dropout_rate': 0.5,  \n",
    "    'validation_accuracy': best_val_acc,\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'training_epochs': fine_tuning_epochs,\n",
    "    'optimizer': 'AdamW',\n",
    "    'learning_rate': 5e-06,\n",
    "    'class_names': class_names,\n",
    "    'timestamp': timestamp\n",
    "}, model_path)\n",
    "\n",
    "print(f\"Model snimljen u: {model_path}\")\n",
    "\n",
    "\n",
    "summary = {\n",
    "    \"projekt_info\": {\n",
    "        \"naziv\": \"Alzheimer's Klasifikacija s Progressive Unfreezing\",\n",
    "        \"datum\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"autor\": \"Lana\",\n",
    "        \"opis\": \"ResNet50 model za klasifikaciju stadija demencije\"\n",
    "    },\n",
    "    \"model_arhitektura\": {\n",
    "        \"tip_modela\": \"AlzheimersResNet50\",\n",
    "        \"bazni_model\": \"ResNet50 (pretrained)\",\n",
    "        \"broj_klasa\": 4,\n",
    "        \"dropout_rate\": 0.5,\n",
    "        \"ukupno_parametara\": int(total_params),\n",
    "        \"trenabilni_parametri\": int(trainable_params),\n",
    "        \"postotak_trenabilnih\": round(trainable_params/total_params*100, 1)\n",
    "    },\n",
    "    \"podaci\": {\n",
    "        \"izvor\": \"Data folder\",\n",
    "        \"klase\": class_names,\n",
    "        \"train_velicina\": len(train_dataset),\n",
    "        \"validation_velicina\": len(val_dataset),\n",
    "        \"test_velicina\": len(test_dataset),\n",
    "        \"batch_size\": batch_size,\n",
    "        \"podjela\": \"70-20-10 (train-val-test)\"\n",
    "    },\n",
    "    \"trening_parametri\": {\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"learning_rate_pocetni\": 5e-06,\n",
    "        \"learning_rate_finalni\": 4e-06,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"broj_epoha\": fine_tuning_epochs,\n",
    "        \"scheduler\": \"ReduceLROnPlateau\",\n",
    "        \"strategija\": \"Progressive Unfreezing\",\n",
    "        \"unfreezing_epoha\": 6,\n",
    "        \"gradient_clipping\": 1.0\n",
    "    },\n",
    "    \"rezultati\": {\n",
    "        \"validation_accuracy\": round(best_val_acc, 2),\n",
    "        \"test_accuracy\": round(test_accuracy, 2),\n",
    "        \"poboljsanje_od_pocetka\": round(test_accuracy - 78.33, 2),\n",
    "        \"prosjecna_precision\": round(float(np.mean(precision)), 3),\n",
    "        \"prosjecna_recall\": round(float(np.mean(recall)), 3),\n",
    "        \"prosjecna_f1\": round(float(np.mean(f1)), 3),\n",
    "        \"generalizacija_gap\": round(best_val_acc - test_accuracy, 2)\n",
    "    },\n",
    "    \"rezultati_po_klasama\": {\n",
    "        class_names[i]: {\n",
    "            \"precision\": round(float(precision[i]), 3),\n",
    "            \"recall\": round(float(recall[i]), 3),\n",
    "            \"f1_score\": round(float(f1[i]), 3),\n",
    "            \"support\": int(support[i])\n",
    "        } for i in range(len(class_names))\n",
    "    },\n",
    "    \"model_status\": {\n",
    "        \"spremnost\": \"PRODUKCIJSKI READY\" if test_accuracy >= 85 else \"POTREBNO DODATNO PODESAVANJE\",\n",
    "        \"medicinska_aplikabilnost\": \"IZVRSNO\" if np.mean(f1) >= 0.9 else \"VRLO DOBRO\" if np.mean(f1) >= 0.8 else \"DOBRO\",\n",
    "        \"preporuke\": [\n",
    "            \"Model je spreman za klinicku uporabu s nadzorem\" if test_accuracy >= 90 else \"Model je spreman za screening aplikacije\",\n",
    "            \"Preporuca se dodatna validacija na vanjskim datasetima\",\n",
    "            \"Potrebno je redovito pracenje performansi u produkciji\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "summary_path = os.path.join(model_dir, f\"{model_name}_summary.json\")\n",
    "with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Sazetak snimljen u: {summary_path}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINALNI SAZETAK PROJEKTA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"POSTIGNUTO: {test_accuracy:.2f}% test accuracy\")\n",
    "\n",
    "\n",
    "print(f\"\\nKLJUCNI POKAZATELJI:\")\n",
    "print(f\"  â€¢ Poboljsanje: +{test_accuracy-78.33:.2f} postotnih bodova\")\n",
    "print(f\"  â€¢ Generalizacija: {'Izvrsna' if abs(best_val_acc - test_accuracy) < 2 else 'Dobra' if abs(best_val_acc - test_accuracy) < 5 else 'Umjerena'}\")\n",
    "print(f\"  â€¢ Prosjecni F1-Score: {np.mean(f1):.3f}\")\n",
    "print(f\"  â€¢ Model robusnos: {'Visoka' if np.mean(confidence_scores) > 0.8 else 'Umjerena'}\")\n",
    "\n",
    "print(f\"\\nMEDICINSKA VAZNOST:\")\n",
    "print(f\"  â€¢ Preciznost dijagnoze: {np.mean(precision)*100:.1f}%\")\n",
    "print(f\"  â€¢ Osjetljivost (recall): {np.mean(recall)*100:.1f}%\")\n",
    "print(f\"  â€¢ Balansiranost klasa: {'Da' if max(f1) - min(f1) < 0.2 else 'Ne'}\")\n",
    "\n",
    "print(f\"\\nPRODUKCIJSKA SPREMNOST:\")\n",
    "print(f\"  â€¢ Status: {summary['model_status']['spremnost']}\")\n",
    "print(f\"  â€¢ Preporuce za: {summary['model_status']['medicinska_aplikabilnost']} uporabu\")\n",
    "\n",
    "print(f\"\\nSNIMLJENI FAJLOVI:\")\n",
    "print(f\"  â€¢ Model: {model_path}\")\n",
    "print(f\"  â€¢ Sazetak: {summary_path}\")\n",
    "\n",
    "print(f\"Model je spreman za implementaciju i moze se koristiti za\")\n",
    "print(f\"automatsku klasifikaciju stadija demencije iz MRI slika!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cb7127",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TESTIRANJE FINALNOG MODELA NA TEST SKUPU PODATAKA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Definiraj nazive klasa\n",
    "class_names = ['Mild Dementia', 'Moderate Dementia', 'Non Demented', 'Very mild Dementia']\n",
    "\n",
    "print(f\"Model: {sgd_model.__class__.__name__}\")\n",
    "print(f\"Najbolja validation accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"Test skup: {len(test_loader)} bathes, {len(test_dataset)} slika\")\n",
    "\n",
    "# TESTIRANJE NA TEST SKUPU\n",
    "sgd_model.eval()\n",
    "test_predictions = []\n",
    "test_labels = []\n",
    "test_probabilities = []\n",
    "\n",
    "print(\"\\nPokretanje testiranja...\")\n",
    "with torch.no_grad():\n",
    "    test_bar = tqdm(test_loader, desc='Testiranje')\n",
    "    \n",
    "    for images, labels in test_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = sgd_model(images)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        test_predictions.extend(predicted.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        test_probabilities.extend(probabilities.cpu().numpy())\n",
    "\n",
    "\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions) * 100\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    test_labels, test_predictions, average=None, zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"REZULTATI TESTIRANJA\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"Broj testiranih slika: {len(test_labels)}\")\n",
    "\n",
    "# Per-class rezultati\n",
    "print(f\"\\nRezultati po klasama:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"  {class_name}:\")\n",
    "    print(f\"    Precision: {precision[i]:.3f}\")\n",
    "    print(f\"    Recall: {recall[i]:.3f}\")\n",
    "    print(f\"    F1-Score: {f1[i]:.3f}\")\n",
    "    print(f\"    Support: {support[i]} slika\")\n",
    "\n",
    "# ProsjeÄne metrije\n",
    "avg_precision = np.mean(precision)\n",
    "avg_recall = np.mean(recall)\n",
    "avg_f1 = np.mean(f1)\n",
    "\n",
    "print(f\"\\nProsjeÄne metrije:\")\n",
    "print(f\"  ProsjeÄna Precision: {avg_precision:.3f}\")\n",
    "print(f\"  ProsjeÄna Recall: {avg_recall:.3f}\")\n",
    "print(f\"  ProsjeÄna F1-Score: {avg_f1:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAÅ½ETAK PERFORMANSI\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"Razlika Val-Test: {best_val_acc - test_accuracy:.2f}%\")\n",
    "\n",
    "if abs(best_val_acc - test_accuracy) < 2:\n",
    "    print(\"  razlika < 2%\")\n",
    "elif abs(best_val_acc - test_accuracy) < 5:\n",
    "    print(\" razlika < 5%\")\n",
    "else:\n",
    "    print(\"MoguÄ‡i overfitting - razlika > 5%\")\n",
    "\n",
    "print(f\"\\n FINALNO:\")\n",
    "print(f\"   PoÄetna accuracy: 78.33%\")\n",
    "print(f\"   Finalna test accuracy: {test_accuracy:.2f}%\") \n",
    "print(f\"   PoboljÅ¡anje: +{test_accuracy - 78.33:.2f}%\")\n",
    "\n",
    "if test_accuracy >= 90:\n",
    "    print(\"Test accuracy >= 90%\")\n",
    "elif test_accuracy >= 85:\n",
    "    print(\" Test accuracy >= 85%\")\n",
    "elif test_accuracy >= 80:\n",
    "    print(\"Test accuracy >= 80%\")\n",
    "else:\n",
    "    print(\"potrebno dodatno podeÅ¡avanje\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda7096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CONFUSION MATRIX I DETALJNE VIZUALIZACIJE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "cm = confusion_matrix(test_labels, test_predictions)\n",
    "cm_normalized = confusion_matrix(test_labels, test_predictions, normalize='true')\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Detaljne Analize Modela - Test Rezultati', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Confusion Matrix (apsolutne vrijednosti)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            ax=axes[0,0])\n",
    "axes[0,0].set_title('Confusion Matrix (Brojevi)')\n",
    "axes[0,0].set_ylabel('Stvarne Klase')\n",
    "axes[0,0].set_xlabel('PredviÄ‘ene Klase')\n",
    "\n",
    "# 2. Confusion Matrix (postotci)\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Oranges',\n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            ax=axes[0,1])\n",
    "axes[0,1].set_title('Confusion Matrix (Postotci)')\n",
    "axes[0,1].set_ylabel('Stvarne Klase')\n",
    "axes[0,1].set_xlabel('PredviÄ‘ene Klase')\n",
    "\n",
    "\n",
    "metrics_data = {\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1\n",
    "}\n",
    "x_pos = np.arange(len(class_names))\n",
    "bar_width = 0.25\n",
    "\n",
    "for i, (metric_name, values) in enumerate(metrics_data.items()):\n",
    "    axes[1,0].bar(x_pos + i*bar_width, values, bar_width, \n",
    "                  label=metric_name, alpha=0.8)\n",
    "\n",
    "axes[1,0].set_xlabel('Klase')\n",
    "axes[1,0].set_ylabel('Score')\n",
    "axes[1,0].set_title('Metrike po Klasama')\n",
    "axes[1,0].set_xticks(x_pos + bar_width)\n",
    "axes[1,0].set_xticklabels([name.split()[0] for name in class_names], rotation=45)\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "if 'val_accuracies' in locals() and len(val_accuracies) > 0:\n",
    "    epochs = range(1, len(val_accuracies) + 1)\n",
    "    axes[1,1].plot(epochs, val_accuracies, 'o-', color='red', linewidth=2, \n",
    "                   markersize=6, label=f'Validation (Max: {max(val_accuracies):.2f}%)')\n",
    "    if 'train_accuracies' in locals():\n",
    "        axes[1,1].plot(epochs, train_accuracies, 's-', color='blue', linewidth=2,\n",
    "                       markersize=4, alpha=0.7, label=f'Training (Final: {train_accuracies[-1]:.2f}%)')\n",
    "    \n",
    "    axes[1,1].axhline(y=88, color='green', linestyle='--', alpha=0.7, label='Cilj (88%)')\n",
    "    axes[1,1].axhline(y=90, color='orange', linestyle='--', alpha=0.7, label='Izvrsno (90%)')\n",
    "    axes[1,1].set_xlabel('Epoha')\n",
    "    axes[1,1].set_ylabel('Accuracy (%)')\n",
    "    axes[1,1].set_title('Training Progress')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "else:\n",
    "\n",
    "    summary_text = f\"\"\"\n",
    "    SAÅ½ETAK REZULTATA:\n",
    "    \n",
    "    Validation Accuracy: {best_val_acc:.2f}%\n",
    "    Test Accuracy: {test_accuracy:.2f}%\n",
    "    \n",
    "    PoboljÅ¡anje: +{test_accuracy - 78.33:.2f}%\n",
    "    \n",
    "    Model Status: {'PRODUKCIJSKI READY' if test_accuracy >= 85 else 'POTREBNO PODEÅ AVANJE'}\n",
    "    \"\"\"\n",
    "    axes[1,1].text(0.1, 0.5, summary_text, transform=axes[1,1].transAxes,\n",
    "                   fontsize=12, verticalalignment='center',\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
    "    axes[1,1].set_title('SaÅ¾etak Performansi')\n",
    "    axes[1,1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DODATNE ANALIZE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "print(\"NajÄeÅ¡Ä‡e greÅ¡ke:\")\n",
    "for i in range(len(class_names)):\n",
    "    for j in range(len(class_names)):\n",
    "        if i != j and cm[i,j] > 0:\n",
    "            error_rate = cm[i,j] / np.sum(cm[i,:]) * 100\n",
    "            if error_rate > 5:  # PrikaÅ¾i samo znaÄajne greÅ¡ke\n",
    "                print(f\"  {class_names[i]} â†’ {class_names[j]}: {cm[i,j]} slika ({error_rate:.1f}%)\")\n",
    "\n",
    "\n",
    "best_class_idx = np.argmax(f1)\n",
    "worst_class_idx = np.argmin(f1)\n",
    "\n",
    "print(f\"\\nNajbolja klasa: {class_names[best_class_idx]} (F1: {f1[best_class_idx]:.3f})\")\n",
    "print(f\"Najslabija klasa: {class_names[worst_class_idx]} (F1: {f1[worst_class_idx]:.3f})\")\n",
    "\n",
    "\n",
    "confidence_scores = np.max(test_probabilities, axis=1)\n",
    "avg_confidence = np.mean(confidence_scores)\n",
    "low_confidence_count = np.sum(confidence_scores < 0.7)\n",
    "\n",
    "print(f\"\\nModel pouzdanost:\")\n",
    "print(f\"  ProsjeÄna pouzdanost: {avg_confidence:.3f}\")\n",
    "print(f\"  Broj predikcija s pouzdanosti < 70%: {low_confidence_count}/{len(confidence_scores)} ({low_confidence_count/len(confidence_scores)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nAnaliza zavrÅ¡ena! Model je spreman za produkciju! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4705436",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DETALJNE ANALIZE PERFORMANSI PO KLASAMA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Generiraj classification report\n",
    "report = classification_report(test_labels, test_predictions, \n",
    "                             target_names=class_names, output_dict=True)\n",
    "\n",
    "print(\"CLASSIFICATION REPORT:\")\n",
    "print(classification_report(test_labels, test_predictions, target_names=class_names))\n",
    "\n",
    "# Detaljne analize po klasama\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DETALJNE ANALIZE PO KLASAMA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"\\nKLASA: {class_name}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Osnovne statistike\n",
    "    tp = cm[i, i]  # True positives\n",
    "    fp = np.sum(cm[:, i]) - tp  # False positives\n",
    "    fn = np.sum(cm[i, :]) - tp  # False negatives\n",
    "    tn = np.sum(cm) - tp - fp - fn  # True negatives\n",
    "    \n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    sensitivity = recall[i]  # Recall = Sensitivity\n",
    " \n",
    "    print(f\"    Broj slika u testu: {support[i]}\")\n",
    "    print(f\"    ToÄno klasificiranih: {tp}\")\n",
    "    print(f\"    PogreÅ¡no kao ova klasa: {fp}\")\n",
    "    print(f\"    PropuÅ¡tenih: {fn}\")\n",
    "  \n",
    "    print(f\"    Precision: {precision[i]:.3f} ({precision[i]*100:.1f}%)\")\n",
    "    print(f\"    Recall (Sensitivity): {recall[i]:.3f} ({recall[i]*100:.1f}%)\")\n",
    "    print(f\"    F1-Score: {f1[i]:.3f}\")\n",
    "    print(f\"    Specificity: {specificity:.3f} ({specificity*100:.1f}%)\")\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    if precision[i] < 0.8 and recall[i] >= 0.8:\n",
    "        print(f\" Puno false positives - model prekasno dijagnostificira {class_name}\")\n",
    "    elif precision[i] >= 0.8 and recall[i] < 0.8:\n",
    "        print(f\" Puno false negatives - model propuÅ¡ta sluÄajeve {class_name}\")\n",
    "    elif precision[i] < 0.8 and recall[i] < 0.8:\n",
    "        print(f\" Potrebno viÅ¡e podataka ili bolje znaÄajke za {class_name}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANALIZA NAJÄŒEÅ Ä†IH GREÅ AKA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "confusion_pairs = []\n",
    "for i in range(len(class_names)):\n",
    "    for j in range(len(class_names)):\n",
    "        if i != j and cm[i,j] > 0:\n",
    "            error_rate = cm[i,j] / np.sum(cm[i,:])\n",
    "            confusion_pairs.append((error_rate, cm[i,j], i, j))\n",
    "\n",
    "\n",
    "confusion_pairs.sort(reverse=True)\n",
    "\n",
    "print(\"Top 5 najproblematiÄnijih parova:\")\n",
    "for idx, (error_rate, count, i, j) in enumerate(confusion_pairs[:5]):\n",
    "    print(f\"{idx+1}. {class_names[i]} â†’ {class_names[j]}\")\n",
    "    print(f\"   Broj greÅ¡aka: {count}\")\n",
    "    print(f\"   Postotak greÅ¡aka: {error_rate*100:.1f}%\")\n",
    "    \n",
    "\n",
    "    if 'Mild' in class_names[i] and 'Very mild' in class_names[j]:\n",
    "        print(\" Bliske faze demencije - teÅ¡ke za razlikovanje\")\n",
    "    elif 'Non Demented' in class_names[i] and 'Very mild' in class_names[j]:\n",
    "        print(\" Rane faze mogu biti sliÄne normalnom stanju\")\n",
    "    elif 'Moderate' in class_names[i] and 'Mild' in class_names[j]:\n",
    "        print(\" Preklapanje simptoma izmeÄ‘u faza\")\n",
    "    print()\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SVEUKUPNI SAÅ½ETAK MODELA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Precision': np.mean(precision),\n",
    "    'Recall': np.mean(recall), \n",
    "    'F1-Score': np.mean(f1),\n",
    "    'Accuracy': test_accuracy/100\n",
    "}\n",
    "\n",
    "print(\"ProsjeÄne performanse:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.3f} ({value*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nModel Status:\")\n",
    "if test_accuracy >= 90:\n",
    "    print(\" Spreman za kliniÄku upotrebu s nadzorem\")\n",
    "elif test_accuracy >= 85:\n",
    "    print(\" OdliÄne performanse za medicinsku dijagnostiku\")\n",
    "elif test_accuracy >= 80:\n",
    "    print(\" Solidne performanse, moguca dodatna optimizacija\")\n",
    "else:\n",
    "    print(\" Potrebna dodatna optimizacija\")\n",
    "\n",
    "print(f\"\\nNapredak:\")\n",
    "print(f\"  PoÄetna accuracy: 78.33%\")\n",
    "print(f\"  Finalna accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"  PoboljÅ¡anje: +{test_accuracy-78.33:.2f} postotnih bodova\")\n",
    "\n",
    "print(f\"\\n Medicinska aplikabilnost:\")\n",
    "if avg_metrics['F1-Score'] >= 0.9:\n",
    "    print(\" IZVRSNO za pomoÄ‡ u dijagnozi\")\n",
    "elif avg_metrics['F1-Score'] >= 0.8:\n",
    "    print(\" VRLO DOBRO za screening\")\n",
    "elif avg_metrics['F1-Score'] >= 0.7:\n",
    "    print(\" KORISNO kao pomoÄ‡ni alat\")\n",
    "else:\n",
    "    print(\" Potrebno poboljÅ¡anje za kliniÄku upotrebu\")\n",
    "\n",
    "print(\"\\nModel je uspjeÅ¡no razvijen i testiran!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d78f189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H5 EXPORT SA NAJBOLJIM MODELOM (93.75% ACCURACY)\n",
    "import os\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "print(\"H5 EXPORT SA NAJBOLJIM MODELOM\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. DEVICE SETUP\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# 2. MODEL CLASS DEFINITION\n",
    "class AlzheimersResNet50(nn.Module):\n",
    "    def __init__(self, num_classes=4, pretrained=True, dropout_rate=0.5):\n",
    "        super(AlzheimersResNet50, self).__init__()\n",
    "        \n",
    "        # Koristi ResNet50 baseline\n",
    "        self.resnet = models.resnet50(pretrained=pretrained)\n",
    "        \n",
    "        # Custom classifier sa regularizacijom\n",
    "        num_features = self.resnet.fc.in_features\n",
    "        \n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# 3. UÄŒITAJ NAJBOLJI MODEL SA 93.75% ACCURACY\n",
    "best_model_path = r\"saved_models\\alzheimers_resnet50_20250809_165610.pth\"\n",
    "\n",
    "print(f\"UÄitavam najbolji model: {best_model_path}\")\n",
    "\n",
    "# Kreiraj model i uÄitaj teÅ¾ine\n",
    "model_to_export = AlzheimersResNet50(num_classes=4, dropout_rate=0.5)\n",
    "\n",
    "try:\n",
    "    # UÄitaj kompletan checkpoint\n",
    "    checkpoint = torch.load(best_model_path, map_location=device)\n",
    "    \n",
    "    # Provjeri strukturu checkpoint-a\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        # Struktura s metadatima\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "        best_validation_acc = checkpoint.get('validation_accuracy', 93.65)\n",
    "        best_test_acc = checkpoint.get('test_accuracy', 93.75)\n",
    "        print(f\"UÄitan checkpoint sa metadatima\")\n",
    "    else:\n",
    "        # Direktan state_dict\n",
    "        state_dict = checkpoint\n",
    "        best_validation_acc = 93.65\n",
    "        best_test_acc = 93.75\n",
    "        print(f\"UÄitan direktan state_dict\")\n",
    "    \n",
    "    # UÄitaj state_dict u model\n",
    "    model_to_export.load_state_dict(state_dict)\n",
    "    model_to_export = model_to_export.to(device)\n",
    "    model_to_export.eval()\n",
    "    \n",
    "    print(f\"USPJEÅ NO UÄŒITAN!\")\n",
    "    print(f\"  Validation accuracy: {best_validation_acc}%\")\n",
    "    print(f\"  Test accuracy: {best_test_acc}%\")\n",
    "    print(f\"  Status: PRODUKCIJSKI READY\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"GreÅ¡ka uÄitavanja modela: {e}\")\n",
    "    print(\"Koristim osnovni model...\")\n",
    "    model_to_export = model_to_export.to(device)\n",
    "    best_validation_acc = 0.0\n",
    "    best_test_acc = 0.0\n",
    "\n",
    "# 4. H5 EXPORT SETUP\n",
    "export_dir = \"exported_models\"\n",
    "if not os.path.exists(export_dir):\n",
    "    os.makedirs(export_dir)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "h5_path = os.path.join(export_dir, f\"alzheimers_BEST_model_{timestamp}.h5\")\n",
    "\n",
    "print(f\"\\nKreiranje NAJBOLJEG H5 modela: {os.path.basename(h5_path)}\")\n",
    "\n",
    "# 5. H5 DATOTEKA KREIRANJE\n",
    "with h5py.File(h5_path, 'w') as h5f:\n",
    "    # Metadata grupa sa PRAVOM accuracy\n",
    "    meta_group = h5f.create_group('metadata')\n",
    "    meta_group.attrs['model_name'] = 'AlzheimersResNet50_BEST'.encode('utf-8')\n",
    "    meta_group.attrs['num_classes'] = 4\n",
    "    meta_group.attrs['validation_accuracy'] = best_validation_acc\n",
    "    meta_group.attrs['test_accuracy'] = best_test_acc  # PRAVA ACCURACY!\n",
    "    meta_group.attrs['timestamp'] = timestamp.encode('utf-8')\n",
    "    meta_group.attrs['pytorch_version'] = torch.__version__.encode('utf-8')\n",
    "    meta_group.attrs['training_date'] = '2025-08-09'.encode('utf-8')\n",
    "    meta_group.attrs['status'] = 'PRODUKCIJSKI_READY'.encode('utf-8')\n",
    "    \n",
    "    # Klase\n",
    "    classes = ['Mild Dementia', 'Moderate Dementia', 'Non Demented', 'Very mild Dementia']\n",
    "    meta_group.create_dataset('classes', data=[s.encode('utf-8') for s in classes])\n",
    "    \n",
    "    # Model parametri grupa\n",
    "    params_group = h5f.create_group('model_parameters')\n",
    "    \n",
    "    print(\"Spremam parametre NAJBOLJEG modela...\")\n",
    "    param_count = 0\n",
    "    for name, param in model_to_export.state_dict().items():\n",
    "        # Konvertiraj u numpy i spremi\n",
    "        param_data = param.cpu().detach().numpy()\n",
    "        \n",
    "        # Provjeri da li je scalar (0D) - ne moÅ¾e biti kompresovan\n",
    "        if param_data.ndim == 0:\n",
    "            params_group.create_dataset(name, data=param_data)\n",
    "        else:\n",
    "            params_group.create_dataset(name, data=param_data, compression='gzip')\n",
    "        param_count += 1\n",
    "    \n",
    "    print(f\"Spremljeno {param_count} parametara\")\n",
    "    \n",
    "    # Model arhitektura info\n",
    "    arch_group = h5f.create_group('architecture')\n",
    "    arch_group.attrs['base_model'] = 'ResNet50'.encode('utf-8')\n",
    "    arch_group.attrs['pretrained'] = True\n",
    "    arch_group.attrs['dropout_rate'] = 0.5\n",
    "    arch_group.attrs['input_size'] = [224, 224, 3]\n",
    "    arch_group.attrs['trainable_params'] = 17528580\n",
    "    arch_group.attrs['total_params'] = 24690500\n",
    "    \n",
    "    # Preprocessing info\n",
    "    preprocess_group = h5f.create_group('preprocessing')\n",
    "    preprocess_group.attrs['input_size'] = [224, 224]\n",
    "    preprocess_group.attrs['mean'] = [0.485, 0.456, 0.406]\n",
    "    preprocess_group.attrs['std'] = [0.229, 0.224, 0.225]\n",
    "    preprocess_group.attrs['normalize'] = True\n",
    "\n",
    "print(f\"\\nSUCCESS - NAJBOLJI MODEL EXPORTIRAN!\")\n",
    "print(f\"H5 model spremljen: {h5_path}\")\n",
    "print(f\"Model validation accuracy: {best_validation_acc}%\")\n",
    "print(f\"Model test accuracy: {best_test_acc}%\")\n",
    "print(f\"Klase: {len(classes)}\")\n",
    "print(f\"VeliÄina datoteke: {os.path.getsize(h5_path) / (1024*1024):.1f} MB\")\n",
    "\n",
    "# Provjera datoteke\n",
    "print(f\"\\nPROVJERA H5 DATOTEKE:\")\n",
    "try:\n",
    "    with h5py.File(h5_path, 'r') as verify_h5f:\n",
    "        print(f\"   Metadata: {len(verify_h5f['metadata'].attrs)} atributa\")\n",
    "        print(f\"   Parametri: {len(verify_h5f['model_parameters'])} tensora\")\n",
    "        print(f\"   Arhitektura: {len(verify_h5f['architecture'].attrs)} podataka\")\n",
    "        print(f\"   Preprocessing: {len(verify_h5f['preprocessing'].attrs)} postavki\")\n",
    "        \n",
    "        # Provjeri accuracy u metadati\n",
    "        val_acc = verify_h5f['metadata'].attrs['validation_accuracy']\n",
    "        test_acc = verify_h5f['metadata'].attrs['test_accuracy']\n",
    "        print(f\"   POTVRÄENO: Val accuracy: {val_acc}%, Test accuracy: {test_acc}%\")\n",
    "        print(\"   H5 datoteka je valjana sa PRAVOM accuracy!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"GreÅ¡ka provjere: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5db5f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST H5 MODELA NA SLIKAMA IZ DATA FOLDERA\n",
    "import h5py\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"TESTIRANJE H5 MODELA\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. UÄŒITAJ H5 MODEL\n",
    "h5_model_path = r\"exported_models\\alzheimers_BEST_model_20250814_161222.h5\"\n",
    "\n",
    "print(f\"UÄitavam H5 model: {h5_model_path}\")\n",
    "\n",
    "# UÄitaj model metadata\n",
    "with h5py.File(h5_model_path, 'r') as h5f:\n",
    "    # ÄŒitaj metadata\n",
    "    val_acc = h5f['metadata'].attrs['validation_accuracy']\n",
    "    test_acc = h5f['metadata'].attrs['test_accuracy']\n",
    "    \n",
    "    # ÄŒitaj klase\n",
    "    classes_bytes = h5f['metadata']['classes'][:]\n",
    "    classes = [cls.decode('utf-8') for cls in classes_bytes]\n",
    "    \n",
    "    # ÄŒitaj preprocessing informacije\n",
    "    mean = h5f['preprocessing'].attrs['mean']\n",
    "    std = h5f['preprocessing'].attrs['std']\n",
    "    input_size = h5f['preprocessing'].attrs['input_size']\n",
    "    \n",
    "    print(f\" Model uÄitan: Val acc {val_acc:.2f}%, Test acc {test_acc:.2f}%\")\n",
    "    print(f\" Klase: {classes}\")\n",
    "    print(f\" Input size: {input_size}\")\n",
    "\n",
    "# 2. REKONSTRUIRAJ PYTORCH MODEL\n",
    "class AlzheimersResNet50(torch.nn.Module):\n",
    "    def __init__(self, num_classes=4, pretrained=True, dropout_rate=0.5):\n",
    "        super(AlzheimersResNet50, self).__init__()\n",
    "        \n",
    "        # Koristi ResNet50 baseline\n",
    "        self.resnet = models.resnet50(pretrained=pretrained)\n",
    "        \n",
    "        # Custom classifier sa regularizacijom\n",
    "        num_features = self.resnet.fc.in_features\n",
    "        \n",
    "        self.resnet.fc = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(dropout_rate),\n",
    "            torch.nn.Linear(num_features, 512),\n",
    "            torch.nn.BatchNorm1d(512),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(512, 256),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Kreiraj model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AlzheimersResNet50(num_classes=4, dropout_rate=0.5).to(device)\n",
    "\n",
    "# UÄitaj parametre iz H5\n",
    "print(\"UÄitavam parametre iz H5...\")\n",
    "with h5py.File(h5_model_path, 'r') as h5f:\n",
    "    state_dict = {}\n",
    "    for param_name in h5f['model_parameters']:\n",
    "        dataset = h5f['model_parameters'][param_name]\n",
    "        \n",
    "\n",
    "        if dataset.shape == ():  # Scalar\n",
    "            param_data = dataset[()]  # UÄitaj scalar vrijednost\n",
    "        else:  # Array\n",
    "            param_data = dataset[:]  # UÄitaj cijeli array\n",
    "            \n",
    "        state_dict[param_name] = torch.from_numpy(np.array(param_data))\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "print(\"âœ“ Model rekonstruiran i spreman za testiranje!\")\n",
    "\n",
    "# 3. PREPROCESSING TRANSFORMACIJE\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "# 4. FUNKCIJA ZA PREDIKCIJU\n",
    "def predict_image(image_path, model, transform, classes, device):\n",
    "    \"\"\"Predvidi klasu za jednu sliku\"\"\"\n",
    "    try:\n",
    "        # UÄitaj sliku\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Preprocess\n",
    "        input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Predikcija\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_tensor)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            predicted_idx = torch.argmax(outputs, dim=1).item()\n",
    "            confidence = probabilities[0][predicted_idx].item()\n",
    "        \n",
    "        # Rezultati\n",
    "        predicted_class = classes[predicted_idx]\n",
    "        all_probs = probabilities[0].cpu().numpy()\n",
    "        \n",
    "        return predicted_class, confidence, all_probs, image\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"GreÅ¡ka kod {image_path}: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TESTIRANJE NA RANDOM SLIKAMA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "data_path = \"Data\"\n",
    "test_results = []\n",
    "\n",
    "for class_name in classes:\n",
    "    class_folder = os.path.join(data_path, class_name)\n",
    "    if os.path.exists(class_folder):\n",
    "        # Uzmi random sliku iz foldera\n",
    "        images = [f for f in os.listdir(class_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        if images:\n",
    "            random_image = random.choice(images)\n",
    "            image_path = os.path.join(class_folder, random_image)\n",
    "            \n",
    "            print(f\"\\nTestiram: {class_name}\")\n",
    "            print(f\"Slika: {random_image}\")\n",
    "            \n",
    "            predicted, confidence, probs, img = predict_image(\n",
    "                image_path, model, test_transform, classes, device\n",
    "            )\n",
    "            \n",
    "            if predicted:\n",
    "                # Rezultat\n",
    "                correct = predicted == class_name\n",
    "                status = \"âœ“ TOÄŒNO\" if correct else \"âœ— POGREÅ NO\"\n",
    "                \n",
    "                print(f\"Stvarna klasa: {class_name}\")\n",
    "                print(f\"PredviÄ‘ena: {predicted} ({confidence:.2%}) {status}\")\n",
    "                \n",
    "                # Top 3 predikcije\n",
    "                top_indices = np.argsort(probs)[::-1][:3]\n",
    "                print(\"Top 3 predikcije:\")\n",
    "                for i, idx in enumerate(top_indices):\n",
    "                    print(f\"  {i+1}. {classes[idx]}: {probs[idx]:.2%}\")\n",
    "                \n",
    "                test_results.append({\n",
    "                    'true_class': class_name,\n",
    "                    'predicted': predicted,\n",
    "                    'confidence': confidence,\n",
    "                    'correct': correct,\n",
    "                    'image_path': image_path\n",
    "                })\n",
    "\n",
    "# 6. SAÅ½ETAK TESTIRANJA\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAÅ½ETAK TESTIRANJA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if test_results:\n",
    "    correct_predictions = sum(1 for r in test_results if r['correct'])\n",
    "    total_tests = len(test_results)\n",
    "    accuracy = correct_predictions / total_tests\n",
    "    \n",
    "    print(f\"Testirane slike: {total_tests}\")\n",
    "    print(f\"ToÄne predikcije: {correct_predictions}\")\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"Model validation accuracy: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Po klasama\n",
    "    print(f\"\\nRezultati po klasama:\")\n",
    "    for class_name in classes:\n",
    "        class_results = [r for r in test_results if r['true_class'] == class_name]\n",
    "        if class_results:\n",
    "            class_correct = sum(1 for r in class_results if r['correct'])\n",
    "            avg_conf = np.mean([r['confidence'] for r in class_results])\n",
    "            print(f\"  {class_name}: {class_correct}/{len(class_results)} toÄno, avg confidence: {avg_conf:.2%}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1927a8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTALACIJA H5PY BIBLIOTEKE\n",
    "print(\"=\" * 60)\n",
    "print(\"INSTALACIJA H5PY ZA HDF5 PODRÅ KU\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import h5py\n",
    "    print(\"h5py je veÄ‡ instaliran!\")\n",
    "except ImportError:\n",
    "    print(\"Instaliram h5py...\")\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"h5py\"])\n",
    "        print(\"h5py uspjeÅ¡no instaliran!\")\n",
    "        \n",
    "        # Provjeri instalaciju\n",
    "        import h5py\n",
    "        print(f\"h5py verzija: {h5py.__version__}\")\n",
    "    except Exception as e:\n",
    "        print(f\"GreÅ¡ka pri instalaciji: {e}\")\n",
    "        print(\"PokuÅ¡avam alternativnu instalaciju...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"h5py\", \"--no-cache-dir\"])\n",
    "            import h5py\n",
    "            print(\"h5py uspjeÅ¡no instaliran s alternativnom metodom!\")\n",
    "        except Exception as e2:\n",
    "            print(f\"NeuspjeÅ¡na instalacija: {e2}\")\n",
    "            print(\"Molim ruÄno instalirajte: pip install h5py\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fc3bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BRZA KONFIGURACIJA ZA PROGRESSIVE UNFREEZING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Data transforms (konzistentni sa prethodnim treningom)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.15, 0.15)),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.1))\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset setup\n",
    "data_path = r\"Data\"\n",
    "print(f\"Loading data from: {data_path}\")\n",
    "\n",
    "# Kreiranje dataseta\n",
    "full_dataset = datasets.ImageFolder(root=data_path, transform=train_transform)\n",
    "print(f\"Total images: {len(full_dataset)}\")\n",
    "print(f\"Classes: {full_dataset.classes}\")\n",
    "\n",
    "# Dataset splitting (70-20-10 split)\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.7 * total_size)\n",
    "val_size = int(0.2 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "# Generator za konzistentno dijeljenje\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, [train_size, val_size, test_size], generator=generator\n",
    ")\n",
    "\n",
    "# RazliÄite transformacije za val i test\n",
    "val_dataset.dataset = datasets.ImageFolder(root=data_path, transform=val_transform)\n",
    "test_dataset.dataset = datasets.ImageFolder(root=data_path, transform=val_transform)\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Validation size: {len(val_dataset)}\")  \n",
    "print(f\"Test size: {len(test_dataset)}\")\n",
    "\n",
    "# Data loaders (optimizirani batch size)\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "\n",
    "# Model definition (AlzheimersResNet50 - kao iz prethodnog uspjesnog treninga)\n",
    "class AlzheimersResNet50(nn.Module):\n",
    "    def __init__(self, num_classes=4, pretrained=True, dropout_rate=0.7):\n",
    "        super(AlzheimersResNet50, self).__init__()\n",
    "        \n",
    "        # Koristi ResNet50 baseline\n",
    "        self.resnet = models.resnet50(pretrained=pretrained)\n",
    "        \n",
    "        # ZaleÄ‘i sve slojeve inicijalno (za maksimalnu regularizaciju)\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Custom classifier sa agresivnom regularizacijom\n",
    "        num_features = self.resnet.fc.in_features\n",
    "        \n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),  # Agressiven dropout\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Odmrzni samo finale slojeve inicijalno\n",
    "        for param in self.resnet.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def unfreeze_more_layers(self, percentage=0.3):\n",
    "        \"\"\"Progressive unfreezing method\"\"\"\n",
    "        total_layers = len(list(self.resnet.named_parameters()))\n",
    "        layers_to_unfreeze = int(total_layers * percentage)\n",
    "        \n",
    "        # Unutar unfreeze_more_layers method\n",
    "        all_params = list(self.resnet.named_parameters())\n",
    "        # Unfreeze odozdo naviÅ¡e (od kraja prema poÄetku)\n",
    "        for i in range(min(layers_to_unfreeze, len(all_params))):\n",
    "            name, param = all_params[-(i+1)]  # Kreni od kraja\n",
    "            if 'fc' not in name:  # Ne diraj finalne slojeve (veÄ‡ su unfrozen)\n",
    "                param.requires_grad = True\n",
    "                \n",
    "    def get_trainable_params(self):\n",
    "        \"\"\"Vrati broj parametara koji se treniraju\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Kreiraj model sa istim postavkama kao uspjeni trening\n",
    "print(\"\\nKreiram model...\")\n",
    "model = AlzheimersResNet50(num_classes=4, pretrained=True, dropout_rate=0.7)\n",
    "model = model.to(device)\n",
    "\n",
    "# PrikaÅ¾i informacije o modelu\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = model.get_trainable_params()\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Trainable ratio: {trainable_params/total_params*100:.1f}%\")\n",
    "\n",
    "# Loss function sa label smoothing (kao u prethodnom uspjesnom treningu)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.2)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
